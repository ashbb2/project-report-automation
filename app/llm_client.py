import os
from dotenv import load_dotenv
from typing import Optional

# Load environment variables
load_dotenv()


class LLMClient:
    """
    Abstraction layer for LLM API calls.
    Supports multiple providers and can operate in stub mode for testing.
    """
    
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "stub")
        self.api_key = os.getenv("OPENAI_API_KEY", "")
        self.stub_mode = self.provider == "stub" or not self.api_key
    
    def generate(self, prompt: str, max_tokens: int = 500) -> str:
        """
        Generate text from a prompt using the configured LLM provider.
        
        Args:
            prompt: The input prompt for the LLM
            max_tokens: Maximum tokens to generate
            
        Returns:
            Generated text response
        """
        if self.stub_mode:
            return self._generate_stub(prompt)
        
        if self.provider == "openai":
            return self._generate_openai(prompt, max_tokens)
        
        raise ValueError(f"Unsupported LLM provider: {self.provider}")
    
    def _generate_stub(self, prompt: str) -> str:
        """
        Return a placeholder response for testing without API calls.
        """
        return (
            f"[STUB MODE] This is a placeholder response for the prompt:\n\n"
            f"{prompt[:100]}...\n\n"
            f"In production, this would be generated by the LLM API."
        )
    
    def _generate_openai(self, prompt: str, max_tokens: int) -> str:
        """
        Generate text using OpenAI API.
        """
        try:
            import openai
            openai.api_key = self.api_key
            
            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a professional business consultant writing feasibility reports."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            
            return response.choices[0].message.content.strip()
        
        except ImportError:
            raise ImportError(
                "OpenAI library not installed. Run: pip install openai"
            )
        except Exception as e:
            raise Exception(f"OpenAI API error: {str(e)}")


# Global instance
llm_client = LLMClient()
